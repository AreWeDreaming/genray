CMPIINSERTPOSITION DECLARATION
      include 'mpif.h' 
      integer rank, rank_i, qsize, src, mpi_index
      integer ierr, tag 
      integer status(MPI_STATUS_SIZE)
      double precision start_time, end_time
      double precision mpi_stime, mpi_etime, comm_time 
      integer command 
CMPIINSERTPOSITION INITIALIZATION
      call MPI_INIT(ierr) 
      call MPI_COMM_SIZE(MPI_COMM_WORLD,qsize,ierr)
      call MPI_COMM_RANK(MPI_COMM_WORLD,rank,ierr)
      write(*,*)'qsize,rank,ierr=',qsize,rank,ierr
      tag=0
      if (rank.eq.0) then
        if (qsize.eq.1) then
          stop 'number of processes should be greater than 1'
	endif
CMPIINSERTPOSITION STARTBARRIER
      start_time = MPI_WTIME()
      comm_time = 0.0
      endif  !On rank.eq.0 
      if (rank.ne.0) then 
         write(*,*)'genray_par, before MPI_RECV, rank=',rank
         call MPI_RECV(command,1,MPI_INTEGER,rank-1, 
     +        tag,MPI_COMM_WORLD,status,ierr) 
         write(*,*)'genray_par, after MPI_RECV, rank,ierr=',rank,ierr
      endif  ! On rank.ne.0
CMPIINSERTPOSITION ENDBARRIER
      write(*,*)'ENDBARRIER START, rank=',rank
      if( rank.ne.qsize-1 ) then
         write(*,*)'genray_par, before MPI_SEND, rank,ierr=',rank,ierr
         call MPI_SEND(command,1,MPI_INTEGER,rank+1, 
     +        tag,MPI_COMM_WORLD,ierr) 
         write(*,*)'genray_par, after MPI_SEND, rank,ierr=',rank,ierr
      endif 
      write(*,*)'ENDBARRIER END, rank=',rank
cYuP: Suggests BARRIER here.
      call MPI_BARRIER(MPI_COMM_WORLD,ierr)
      write(*,*)'After MPI_BARRIER 1, ierr=',ierr
      if( rank.eq.0 ) then 
CMPIINSERTPOSITION JUSTBEFORETHELOOP
      endif  !On rank=0
CMPIINSERTPOSITION STARTTHELOOP
      write(*,*) 'iray,rank = ', iray, rank
      if( rank.eq.0 ) then
CMPIINSERTPOSITION PRINTHEADER
      endif  !On rank=0
CMPIINSERTPOSITION ALLOCATESENDRECV
      call sendrecv_alloc(nrayelt,nray,nfreq,nbulk,rank)
cYuP: Suggests BARRIER here.
      call MPI_BARRIER(MPI_COMM_WORLD,ierr)
      write(*,*)'After MPI_BARRIER 2, ierr=',ierr
CMPIINSERTPOSITION STARTEMISSION
      mpi_index=nfreq*(iray-1)+ifreq-1
      rank_i=MOD(mpi_index,qsize-1)+1 
      if ((rank.ne.0).and.(rank.ne.rank_i)) goto 25
      write(*,*) '  iray,rank,ifreq,mpi_index,rank_i = ', 
     +              iray,rank,ifreq,mpi_index,rank_i
c      if( rank.eq.0 ) then
c      endif
CMPIINSERTPOSITION STARTRUNGEKUTTA
      write(*,*)'genray_par, before goto 23, rank=',rank
      if( rank.eq.0 ) goto 23 
      write(*,*)'genray_par, after goto 23, rank=',rank
CMPIINSERTPOSITION ENDRUNGEKUTTA
      write(*,*)'Before senddata, rank=',rank
      call senddata(nrayelt,nray,nfreq,nbulk)
CVARCOMM
      goto 25
 23   mpi_stime = MPI_WTIME()
      call recvdata(nrayelt,nray,nfreq,nbulk,rank_i)
      mpi_etime = MPI_WTIME()
      comm_time = comm_time+(mpi_etime-mpi_stime)
CMPIINSERTPOSITION ENDEMISSION
      if( rank.ne.0 ) goto 25
CMPIINSERTPOSITION SAVEDATA
      if( rank.ne.0 ) then
         goto 20
      endif
CMPIINSERTPOSITION JUSTAFTERTHELOOP
      if( rank.ne.0 ) then 
         goto 100
      endif
      end_time = MPI_WTIME()
      write(*,*) 'full time: ', end_time-start_time
      write(*,*) 'comm time: ', comm_time
CMPIINSERTPOSITION ENDMPI
 100  call MPI_FINALIZE(ierr)
CMPIINSERTPOSITION SUBS

      subroutine sendrecv_alloc(nrayelt,nray,nfreq,nbulk,irank)
c      save
      integer nrayelt, nray, nfreq, nbulk,irank
      include 'param.i'
c     Adjusted for dynamic dimensioning [BH111102].

cBH130508     Re-adjusted, to work.
cBH130508     Furure work: further adjust for dynamic dimensioning.
cBH130508     nbulk argument not needed now, but possibly for dyn dim.
c
      complex*16, dimension(:), allocatable :: c1,c2
      real*8, dimension(:), allocatable :: r1,r2,r3,r4,r5
      integer, dimension(:), allocatable :: i1(),i2()
      common /write/ c1,r1,c2,r2,i1,r3,i2,r4,i3,r5,i4

      complex*16 czero
      real*8 zero
      integer izero
c
      lc1=3*nrelta
      lr1=39*nrelta
      lc2=9*nrelta
      lr2=nrelta*nbulka+1+19*nrelta+3*nray*nfreq+15*nfreq+20*nrelta+6
      li1=3
      lr3=4
      li2=1
      lr4=8*nrelta*(n_relt_harm2a-n_relt_harm1a+1)
      li3=nrelta*(n_relt_harm2a-n_relt_harm1a+1)
      lr5=1+2*nrelta
      li4=2
      write(*,*)'sendrec_alloc: 
     +   lc1,lc2,lr1,lr2,lr3,lr4,lr5,li1,li2,li3,li4 =',
     +   lc1,lc2,lr1,lr2,lr3,lr4,lr5,li1,li2,li3,li4
c
c     Already allocated?
      if (.not.allocated(c2)) then
      write(*,*)'sendrecv_alloc:  allocating, irank=',irank
      istat_tot=0
      allocate(c1(1:lc1),STAT=istat)
      istat_tot=istat_tot+istat
      allocate(c2(1:lc2),STAT=istat)
      istat_tot=istat_tot+istat
      allocate(r1(1:lr1),STAT=istat)
      istat_tot=istat_tot+istat
      allocate(r2(1:lr2),STAT=istat)
      istat_tot=istat_tot+istat
      allocate(r3(1:lr3),STAT=istat)
      istat_tot=istat_tot+istat
      allocate(r4(1:lr4),STAT=istat)
      istat_tot=istat_tot+istat
      allocate(r5(1:lr5),STAT=istat)
      istat_tot=istat_tot+istat
      allocate(i1(1:li1),STAT=istat)
      istat_tot=istat_tot+istat
      allocate(i2(1:li2),STAT=istat)
      istat_tot=istat_tot+istat
      allocate(i3(1:li3),STAT=istat)
      istat_tot=istat_tot+istat
      allocate(i4(1:li4),STAT=istat)
      istat_tot=istat_tot+istat

!      PAUSE


c      
c     Check that allocations were OK
      if (istat_tot.ne.0) then
         write(*,*)'sendrecv_alloc:  Problem with allocation'
         write(*,*)'sendrecv_alloc:  Reduce param.h paramaters?'
         write(*,*)'sendrecv_alloc:  Stopping'
         STOP
      endif
      endif  !On .not.associated(c1)

c     Do the zeroing:
      zero=0d0
      czero=(0d0,0d0)
      izero=0
      call ccast(c1,czero,SIZE(c1))
      call ccast(c2,czero,SIZE(c2))
      call bcast(r1,zero,SIZE(r1))
      call bcast(r2,zero,SIZE(r2))
      call bcast(r3,zero,SIZE(r3))
      call bcast(r4,zero,SIZE(r4))
      call bcast(r5,zero,SIZE(r5))
      call ibcast(i1,izero,SIZE(i1))
      call ibcast(i2,izero,SIZE(i2))
      call ibcast(i4,izero,SIZE(i4))
      call ibcast(i3,izero,SIZE(i3))

      write(*,*)'sendrecv_alloc: i4=',i4
      write(*,*)'sendrecv_alloc: size(i4),irank=',size(i4),irank

c
      return
      end
c      
c      
      subroutine senddata(nrayelt,nray,nfreq,nbulk)
c      save
      integer nrayelt, nray, nfreq, nbulk, tag
      include 'mpif.h'
      include 'param.i'
      integer command
c
c     Adjusted for dynamic dimensioning [BH111102].
cBH130508     Re-adjusted, to work.
c

      complex*16, dimension(:), allocatable :: c1,c2
      real*8, dimension(:), allocatable :: r1,r2,r3,r4,r5
      integer, dimension(:), allocatable :: i1(),i2()
      common /write/ c1,r1,c2,r2,i1,r3,i2,r4,i3,r5,i4
c
      lc1=3*nrelta
      lr1=39*nrelta
      lc2=9*nrelta
      lr2=nrelta*nbulka+1+19*nrelta+3*nray*nfreq+15*nfreq+20*nrelta+6
      li1=3
      lr3=4
      li2=1
      lr4=8*nrelta*(n_relt_harm2a-n_relt_harm1a+1)
      li3=nrelta*(n_relt_harm2a-n_relt_harm1a+1)
      lr5=1+2*nrelta
      li4=2
      write(*,*)'senddata: 
     +   lc1,lc2,lr1,lr2,lr3,lr4,lr5,li1,li2,li3,li4 =',
     +   lc1,lc2,lr1,lr2,lr3,lr4,lr5,li1,li2,li3,li4
c
      tag = 1
      write(*,*)'mpi.ins:  HERE5'
      call MPI_RECV(command,1,MPI_INTEGER,0,
     +     tag,MPI_COMM_WORLD,status,ierr)
      write(*,*)'senddata: command=',command
      call MPI_SEND(nrayelt,1,MPI_INTEGER,0,
     +     1,MPI_COMM_WORLD,ierr)
      write(*,*)'senddata: nrayelt=',nrayelt
      write(*,*)'senddata: before tag=2, size(c1)=',size(c1)
      call MPI_SEND(c1(1),lc1,MPI_DOUBLE_COMPLEX,0,
     +     2,MPI_COMM_WORLD,ierr)
      write(*,*)'senddata: after tag=2, size(c1)=',size(c1)
      call MPI_SEND(r1(1),lr1,MPI_DOUBLE_PRECISION,0,
     +     3,MPI_COMM_WORLD,ierr) 
      write(*,*)'senddata: tag=3, size(r1)=',shape(r1)
      call MPI_SEND(c2(1),lc2,MPI_DOUBLE_COMPLEX,0,
     +     4,MPI_COMM_WORLD,ierr)
      write(*,*)'senddata: tag=4, shape(c2)=',shape(c2)
      call MPI_SEND(r2,lr2,MPI_DOUBLE_PRECISION,0,
     +     5,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(i1,li1,MPI_INTEGER,0,
     +     6,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(r3,lr3,MPI_DOUBLE_PRECISION,0,
     +     7,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(i2,li2,MPI_INTEGER,0,
     +     8,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(r4,lr4,MPI_DOUBLE_PRECISION,0,
     +     9,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(i3,li3,MPI_INTEGER,0,
     +     10,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(r5,lr5,MPI_DOUBLE_PRECISION,0,
     +     11,MPI_COMM_WORLD,ierr) 
      write(*,*)'senddata: i4 before SEND, i4=',i4
      write(*,*)'senddata: i4 before SEND, size(i4)=',size(i4)
      call MPI_SEND(i4,li4,MPI_INTEGER,0,
     +     12,MPI_COMM_WORLD,ierr) 
      write(*,*)'senddata: tag=12, shape(i4)=',shape(ir)
      return 
      end
c
c

      subroutine recvdata(nrayelt,nray,nfreq,nbulk,src)
c      save
      integer nrayelt, nray, nfreq, nbulk, src, tag
      include 'mpif.h'
      include 'param.i'
      integer command
c     Adjusted for dynamic dimensioning [BH111102].
cBH130508     Re-adjusted, to work.
c
      complex*16, dimension(:), allocatable :: c1,c2
      real*8, dimension(:), allocatable :: r1,r2,r3,r4,r5
      integer, dimension(:), allocatable :: i1(),i2()
      common /write/ c1,r1,c2,r2,i1,r3,i2,r4,i3,r5,i4
c
      lc1=3*nrelta
      lr1=39*nrelta
      lc2=9*nrelta
      lr2=nrelta*nbulka+1+19*nrelta+3*nray*nfreq+15*nfreq+20*nrelta+6
      li1=3
      lr3=4
      li2=1
      lr4=8*nrelta*(n_relt_harm2a-n_relt_harm1a+1)
      li3=nrelta*(n_relt_harm2a-n_relt_harm1a+1)
      lr5=1+2*nrelta
      li4=2
      write(*,*)'recvdata: 
     +   lc1,lc2,lr1,lr2,lr3,lr4,lr5,li1,li2,li3,li4 =',
     +   lc1,lc2,lr1,lr2,lr3,lr4,lr5,li1,li2,li3,li4
c
      tag=1
      write(*,*)'mpi.ins:  HERE1'
      call MPI_SEND(command,1,MPI_INTEGER,src,
     +     tag,MPI_COMM_WORLD,ierr)
      write(*,*)'recvdata: command=',command
      call MPI_RECV(nrayelt,1,MPI_INTEGER,src,
     +     1,MPI_COMM_WORLD,status,ierr)
      write(*,*)'recvdata: nrayelt=',nrayelt
      write(*,*)'recvdata: before tag=2, shape(c1)=',shape(c1)
      call MPI_RECV(c1(1),lc1,MPI_DOUBLE_COMPLEX,src,
     +     2,MPI_COMM_WORLD,status,ierr)
      write(*,*)'recvdata: after tag=2, shape(c1)=',shape(c1)
      call MPI_RECV(r1(1),lr1,MPI_DOUBLE_PRECISION,src,
     +     3,MPI_COMM_WORLD,status,ierr)
      write(*,*)'recvdata: tag=3, shape(r1)=',shape(r1)
      call MPI_RECV(c2(1),lc2,MPI_DOUBLE_COMPLEX,src,
     +     4,MPI_COMM_WORLD,status,ierr)
      write(*,*)'recvdata: tag=4, shape(c2)=',shape(c2)
      call MPI_RECV(r2,lr2,MPI_DOUBLE_PRECISION,src,
     +     5,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(i1,li1,MPI_INTEGER,src,
     +     6,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(r3,lr3,MPI_DOUBLE_PRECISION,src,
     +     7,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(i2,li2,MPI_INTEGER,src,
     +     8,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(r4,lr4,MPI_DOUBLE_PRECISION,src,
     +     9,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(i3,li3,MPI_INTEGER,src,
     +     10,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(r5,lr5,MPI_DOUBLE_PRECISION,src,
     +     11,MPI_COMM_WORLD,status,ierr)
      write(*,*)'recvdata: i4 before RECV, i4=',i4
      write(*,*)'recvdata: i4 before RECV, size(i4)=',size(i4)
      call MPI_RECV(i4,li4,MPI_INTEGER,src,
     +     12,MPI_COMM_WORLD,status,ierr)
      write(*,*)'recvdata: tag=12, size(i4)=',shape(i4)
      return 
      end
CMPIINSERTPOSITION
