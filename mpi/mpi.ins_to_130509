CMPIINSERTPOSITION DECLARATION
      include 'mpif.h' 
      integer rank, rank_i, qsize, src, cur_rf 
      integer ierr, tag 
      integer status(MPI_STATUS_SIZE)
      double precision start_time, end_time
      double precision mpi_stime, mpi_etime, comm_time 
      integer command 
CMPIINSERTPOSITION INITIALIZATION
      call MPI_INIT(ierr) 
      call MPI_COMM_SIZE(MPI_COMM_WORLD,qsize,ierr)
      call MPI_COMM_RANK(MPI_COMM_WORLD,rank,ierr)
      write(*,*)'qsize,rank,ierr=',qsize,rank,ierr
      tag=0
      if (rank.eq.0) then
        if (qsize.eq.1) then
          stop 'number of processes should be greater than 1'
	endif
CMPIINSERTPOSITION STARTBARRIER
      start_time = MPI_WTIME()
      comm_time = 0.0
      endif 
      if (rank.ne.0) then 
         call MPI_RECV(command,1,MPI_INTEGER,rank-1, 
     +        tag,MPI_COMM_WORLD,status,ierr) 
      endif 
CMPIINSERTPOSITION ENDBARRIER
      if( rank.ne.qsize-1 ) then 
         call MPI_SEND(command,1,MPI_INTEGER,rank+1, 
     +        tag,MPI_COMM_WORLD,ierr) 
      endif 
      if( rank.eq.0 ) then 
CMPIINSERTPOSITION JUSTBEFORETHELOOP
      endif
CMPIINSERTPOSITION STARTTHELOOP
      if( rank.eq.0 ) then
         write(*,*) 'iray = ', iray
CMPIINSERTPOSITION PRINTHEADER
      endif
CMPIINSERTPOSITION ALLOCATESENDRECV
      call sendrecv_alloc(nrayelt,nray,nfreq,nbulk)
CMPIINSERTPOSITION STARTEMISSION
      cur_rf=nfreq*(iray-1)+ifreq-1
      rank_i=MOD(cur_rf,qsize-1)+1 
      if ((rank.ne.0).and.(rank.ne.rank_i)) goto 25
      if( rank.eq.0 ) then
	 write(*,*) '  ifreq = ', ifreq
      endif
CMPIINSERTPOSITION STARTRUNGEKUTTA
      if( rank.eq.0 ) goto 23 
CMPIINSERTPOSITION ENDRUNGEKUTTA
      call senddata(nrayelt,nray,nfreq,nbulk)
CVARCOMM
      goto 25
 23   mpi_stime = MPI_WTIME()
      call recvdata(nrayelt,nray,nfreq,nbulk,rank_i)
      mpi_etime = MPI_WTIME()
      comm_time = comm_time+(mpi_etime-mpi_stime)
CMPIINSERTPOSITION ENDEMISSION
      if( rank.ne.0 ) goto 25
CMPIINSERTPOSITION SAVEDATA
      if( rank.ne.0 ) then
         goto 20
      endif
CMPIINSERTPOSITION JUSTAFTERTHELOOP
      if( rank.ne.0 ) then 
         goto 100
      endif
      end_time = MPI_WTIME()
      write(*,*) 'full time: ', end_time-start_time
      write(*,*) 'comm time: ', comm_time
CMPIINSERTPOSITION ENDMPI
 100  call MPI_FINALIZE(ierr)
CMPIINSERTPOSITION SUBS
      subroutine sendrecv_alloc(nrayelt,nray,nfreq,nbulk)
      integer nrayelt, nray, nfreq, nbulk
      include 'param.i'
c     Adjusted for dynamic dimensioning [BH111102].
cBH130508     Re-adjusted, to work.
cBH130508     Furure work: further adjust for dynamic dimensioning.
cBH130508     nbulk argument not needed now, but possibly for dyn dim.
c
      complex*16, pointer :: c1(:),c2(:)
      real*8, pointer :: r1(:),r2(:),r3(:),r4(:),r5(:)
      integer, pointer :: i1(:),i2(:),i3(:),i4(:)
      common /write/ c1,r1,c2,r2,i1,r3,i2,r4,i3,r5,i4
c
      lc1=3*nrelta
      lr1=39*nrelta
      lc2=9*nrelta
      lr2=nrelta*nbulka+1+19*nrelta+3*nray*nfreq+15*nfreq+20*nrelta+6
      li1=3
      lr3=4
      li2=1
      lr4=8*nrelta*(n_relt_harm2a-n_relt_harm1a+1)
      li3=nrelta*(n_relt_harm2a-n_relt_harm1a+1)
      lr5=1+2*nrelta
      li4=2
      write(*,*)'sendrec_alloc: 
     +   lc1,lc2,lr1,lr2,lr3,lr4,lr5,li1,li2,li3,li4 =',
     +   lc1,lc2,lr1,lr2,lr3,lr4,lr5,li1,li2,li3,li4
c
      istat_tot=0
      allocate(c1(1:lc1),STAT=istat)
      istat_tot=istat_tot+istat
      call ccast(c1,zero,SIZE(c1))
      allocate(c2(1:lc2),STAT=istat)
      istat_tot=istat_tot+istat
      call ccast(c2,zero,SIZE(c2))
      allocate(r1(1:lr1),STAT=istat)
      istat_tot=istat_tot+istat
      call bcast(r1,zero,SIZE(r1))
      allocate(r2(1:lr2),STAT=istat)
      istat_tot=istat_tot+istat
      call bcast(r2,zero,SIZE(r2))
      allocate(r3(1:lr3),STAT=istat)
      istat_tot=istat_tot+istat
      call bcast(r3,zero,SIZE(r3))
      allocate(r4(1:lr4),STAT=istat)
      istat_tot=istat_tot+istat
      call bcast(r4,zero,SIZE(r4))
      allocate(r5(1:lr5),STAT=istat)
      istat_tot=istat_tot+istat
      call bcast(r5,zero,SIZE(r5))
      allocate(i1(1:li1),STAT=istat)
      istat_tot=istat_tot+istat
      call ibcast(i1,zero,SIZE(i1))
      allocate(i2(1:li2),STAT=istat)
      istat_tot=istat_tot+istat
      call ibcast(i2,zero,SIZE(i2))
      allocate(i3(1:li3),STAT=istat)
      istat_tot=istat_tot+istat
      call ibcast(i3,zero,SIZE(i3))
      allocate(i4(1:li4),STAT=istat)
      istat_tot=istat_tot+istat
      call ibcast(i4,zero,SIZE(i4))
c      
c     Check that allocations were OK
      if (istat_tot.ne.0) then
         write(*,*)'ainalloc.f:  Problem with allocation'
         write(*,*)'ainalloc.f:  Reduce param.h paramaters?'
         write(*,*)'ainalloc.f:  Stopping'
         STOP
      endif
c
      return
      end
c      
c      
      subroutine senddata(nrayelt,nray,nfreq,nbulk)
      integer nrayelt, nray, nfreq, nbulk, tag
      include 'param.i'
c
c     Adjusted for dynamic dimensioning [BH111102].
cBH130508     Re-adjusted, to work.
c
      complex*16, pointer :: c1(:),c2(:)
      real*8, pointer :: r1(:),r2(:),r3(:),r4(:),r5(:)
      integer, pointer :: i1(:),i2(:),i3(:),i4(:)
      common /write/ c1,r1,c2,r2,i1,r3,i2,r4,i3,r5,i4
c
      lc1=3*nrelta
      lr1=39*nrelta
      lc2=9*nrelta
      lr2=nrelta*nbulka+1+19*nrelta+3*nray*nfreq+15*nfreq+20*nrelta+6
      li1=3
      lr3=4
      li2=1
      lr4=8*nrelta*(n_relt_harm2a-n_relt_harm1a+1)
      li3=nrelta*(n_relt_harm2a-n_relt_harm1a+1)
      lr5=1+2*nrelta
      li4=2
      write(*,*)'senddata: 
     +   lc1,lc2,lr1,lr2,lr3,lr4,lr5,li1,li2,li3,li4 =',
     +   lc1,lc2,lr1,lr2,lr3,lr4,lr5,li1,li2,li3,li4
c
      tag = 1
      write(*,*)'mpi.ins:  HERE5'
      call MPI_RECV(command,1,MPI_INTEGER,0,
     +     tag,MPI_COMM_WORLD,status,ierr)
      write(*,*)'mpi.ins:  HERE6'
      call MPI_SEND(nrayelt,1,MPI_INTEGER,0,
     +     tag,MPI_COMM_WORLD,ierr)
      call MPI_SEND(c1,lc1,MPI_DOUBLE_COMPLEX,0,
     +     tag,MPI_COMM_WORLD,ierr)
      call MPI_SEND(r1,lr1,MPI_DOUBLE_PRECISION,0,
     +     tag,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(c2,lc2,MPI_DOUBLE_COMPLEX,0,
     +     tag,MPI_COMM_WORLD,ierr)
      call MPI_SEND(r2,lr2,MPI_DOUBLE_PRECISION,0,
     +     tag,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(i1,li1,MPI_INTEGER,0,
     +     tag,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(r3,lr3,MPI_DOUBLE_PRECISION,0,
     +     tag,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(i2,li2,MPI_INTEGER,0,
     +     tag,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(r4,lr4,MPI_DOUBLE_PRECISION,0,
     +     tag,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(i3,li3,MPI_INTEGER,0,
     +     tag,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(r5,lr5,MPI_DOUBLE_PRECISION,0,
     +     tag,MPI_COMM_WORLD,ierr) 
      call MPI_SEND(i4,li4,MPI_INTEGER,0,
     +     tag,MPI_COMM_WORLD,ierr) 
      return 
      end
c
      subroutine recvdata(nrayelt,nray,nfreq,nbulk,src)
      integer nrayelt, nray, nfreq, nbulk, src, tag
      include 'param.i'
c     Adjusted for dynamic dimensioning [BH111102].
cBH130508     Re-adjusted, to work.
c
      complex*16, pointer :: c1(:),c2(:)
      real*8, pointer :: r1(:),r2(:),r3(:),r4(:),r5(:)
      integer, pointer :: i1(:),i2(:),i3(:),i4(:)
      common /write/ c1,r1,c2,r2,i1,r3,i2,r4,i3,r5,i4
c
      lc1=3*nrelta
      lr1=39*nrelta
      lc2=9*nrelta
      lr2=nrelta*nbulka+1+19*nrelta+3*nray*nfreq+15*nfreq+20*nrelta+6
      li1=3
      lr3=4
      li2=1
      lr4=8*nrelta*(n_relt_harm2a-n_relt_harm1a+1)
      li3=nrelta*(n_relt_harm2a-n_relt_harm1a+1)
      lr5=1+2*nrelta
      li4=2
      write(*,*)'recvdata: 
     +   lc1,lc2,lr1,lr2,lr3,lr4,lr5,li1,li2,li3,li4 =',
     +   lc1,lc2,lr1,lr2,lr3,lr4,lr5,li1,li2,li3,li4
c
      tag=1
      write(*,*)'mpi.ins:  HERE1'
      call MPI_SEND(command,1,MPI_INTEGER,src,
     +     tag,MPI_COMM_WORLD,ierr)
      write(*,*)'mpi.ins:  HERE2'
      call MPI_RECV(nrayelt,1,MPI_INTEGER,src,
     +     tag,MPI_COMM_WORLD,status,ierr)
      write(*,*)'mpi.ins:  HERE3'
      call MPI_RECV(c1,lc1,MPI_DOUBLE_COMPLEX,src,
     +     tag,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(r1,lr1,MPI_DOUBLE_PRECISION,src,
     +     tag,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(c2,lc2,MPI_DOUBLE_COMPLEX,src,
     +     tag,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(r2,lr2,MPI_DOUBLE_PRECISION,src,
     +     tag,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(i1,li1,MPI_INTEGER,src,
     +     tag,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(r3,lr3,MPI_DOUBLE_PRECISION,src,
     +     tag,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(i2,li2,MPI_INTEGER,src,
     +     tag,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(r4,lr4,MPI_DOUBLE_PRECISION,src,
     +     tag,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(i3,li3,MPI_INTEGER,src,
     +     tag,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(r5,lr5,MPI_DOUBLE_PRECISION,src,
     +     tag,MPI_COMM_WORLD,status,ierr)
      call MPI_RECV(i4,li4,MPI_INTEGER,src,
     +     tag,MPI_COMM_WORLD,status,ierr)
      write(*,*)'mpi.ins:  HERE4'
      return 
      end
CMPIINSERTPOSITION
